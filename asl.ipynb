{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import PIL.Image\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, Model\n",
    "from keras.applications.vgg16 import VGG16"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gesture Recognition\n",
    "\n",
    "American Sign Language is a natural language serving as the sign language of Deaf communities in the United States of America, as well as Anglophone Canada.  As with any sign language, it employs hand movements combined with facial expressions and body posture to convey expressions, words, and ideas.\n",
    "\n",
    "This notebook's goal is to create a Convolutional Neural Network that can identify signs made in ASL.  The specific set of signs focused here will be that of the English alphabet which is pictured below.\n",
    "\n",
    "<img src = 'notebook/alphabet_image.png' width=\"300\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code cell sets the dimensions, hyperparameters, and folders for training and testing the models.\n",
    "\n",
    "The data is not included in the repository.  The same data, as specifed in the README, may be acquired or the below strings may be modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "img_height = 200\n",
    "img_width = 200\n",
    "batch_size = 32\n",
    "data_dir = \"data/asl_alphabet_train\"\n",
    "test_dir = \"data/asl_alphabet_test/asl_alphabet_test\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "Both the training and testing data is processed and split using the keras preprocessing function.\n",
    "\n",
    "Split is done 80 / 20 with an arbitrary seed for consistency purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = train_ds.class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is normalized using the keras Rescaling function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data Breakdown"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the training code, the data used for this notebook did not label the test data in a way that is processable by the Keras preprocessing function.\n",
    "\n",
    "This code iterates through all the files in the test directory and properly breaks down the labels inscribed in their filenames according to the dictionary created from the training set classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = os.listdir(test_dir)\n",
    "class_enum = enumerate(classes)\n",
    "class_dict = dict((j, i) for i, j in class_enum)\n",
    "image_array = []\n",
    "label_array = []\n",
    "for item in x:\n",
    "  if item.lower().endswith('.jpg'):\n",
    "    image = tf.io.read_file(test_dir + '/' + item)\n",
    "    image = tf.image.decode_image(image, channels = 3)\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    image = tf.image.resize(image, size=(200, 200))\n",
    "    image_array.append(image)\n",
    "    label = class_dict.get(item[0])\n",
    "    label_array.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_array = np.asarray(image_array)\n",
    "label_array = np.asarray(label_array)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Function\n",
    "\n",
    "Plots the training and validation curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_his(history):\n",
    "  plt.plot(history.history['loss'])\n",
    "  plt.plot(history.history['val_loss'])\n",
    "  plt.title('Model Loss')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.legend(['train', 'validation'])\n",
    "  plt.show()\n",
    "\n",
    "  plt.plot(history.history['accuracy'])\n",
    "  plt.plot(history.history['val_accuracy'])\n",
    "  plt.title('Model Accuracy')\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.legend(['train', 'validation'])\n",
    "  plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Construction\n",
    "\n",
    "This is a small model to test the capabilities of a CNN model in identifying signs.\n",
    "\n",
    "The model is kept relatively small for the purposes of keeping runtime low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(32, (5, 5), \n",
    "                 strides = 3,\n",
    "                 padding = 'Same',\n",
    "                 activation = 'relu',\n",
    "                 input_shape = (200, 200, 3)))\n",
    "\n",
    "model.add(tf.keras.layers.MaxPooling2D())\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Conv2D(32, (3, 3),\n",
    "                 strides = 2,\n",
    "                 padding = 'Same',\n",
    "                 activation = 'relu'))\n",
    "\n",
    "model.add(tf.keras.layers.MaxPooling2D())\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "model.add(tf.keras.layers.Dense(108, activation = 'relu'))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(108, activation = 'relu'))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(59, activation = 'relu'))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(26))\n",
    "\n",
    "model.compile(optimizer = tf.keras.optimizers.Adagrad(learning_rate = 0.1),\n",
    "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics = ['accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stores the best model in the specified path\n",
    "filepath = \"data/best_model.hdf5\"\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath,\n",
    "              monitor='val_accuracy', verbose=1, save_best_only=True)\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "history_3 = model.fit(train_ds, validation_data=val_ds, epochs=epochs,\n",
    "                    callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_his(history_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(image_array, label_array)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simple model above displays a high degree of capability in identifying the set of signs.\n",
    "A larger model should be able to do much better, and these models should be applicable to a larger set of ASL."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a large model for this task is redunant when the task can be applied to an already available model.  For this task, the VGG16 pretrained model from the Keras library will be reapplied to the ASL English alphabet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = VGG16(weights = 'imagenet', \n",
    "                   include_top = False, \n",
    "                   input_shape = (200, 200, 3), \n",
    "                   pooling = None)\n",
    "\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze Base Model\n",
    "for layer in base_model.layers:  \n",
    "    print(layer)\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two hidden layers and an output layer are added for training\n",
    "x = base_model.output\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "x = tf.keras.layers.Dense(108, activation = 'relu')(x)\n",
    "x = tf.keras.layers.Dense(108, activation = 'relu')(x)\n",
    "x = tf.keras.layers.Dense(26)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs = base_model.input, outputs = x)\n",
    "\n",
    "model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001),\n",
    "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics = ['accuracy'])\n",
    "\n",
    "filepath = \"pweights-improvement-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True)\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "history_p = model.fit(train_ds, validation_data=val_ds, epochs=epochs, callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_his(history_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(image_array, label_array)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bf8cdf3c02cf4bcb684ab826931a4533f68b0ad468c214fbb44d0d616d81f39f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
